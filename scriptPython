#!/usr/bin/python
import sys
import string
import nlpnet
import nltk

# return text from file in command line paramater
def get_text_from_file():
  if len(sys.argv) < 2:
    print("Uso: python wordcounter.py <arquivos a serem contados>")
    sys.exit(1)

  text = ''
  for i in range(1,len(sys.argv)):
    filename = sys.argv[i]
    with open(filename) as f:
      textfile = f.read()
      text += '' + textfile
      f.close()
  return text


#--- GET's

# Get every words - with out pontuation - in text
def get_words(text):
  splittext = text.split()
  words = []
  pont = str.maketrans({key: None for key in string.punctuation})
  for word in splittext:
    if not word.isalpha():
      word = word.translate(pont)
    words.append(word)
  return words

# Get text in lines - in \n and .
def get_lines(text):
  return [x for x in map(str.strip, text.split('.')) if x]

# Get words - with out pontuation - in text
def get_diferent_words(text):
  splittext = text.split()
  words = set()
  pont = str.maketrans({key: None for key in string.punctuation})
  for word in splittext:
    if not word.isalpha():
      word = word.translate(pont)
    words.add(word)
  return words

#--- count

# Count total number of words
def count_words(text):
  words = get_words(text)
  return len(words)

# Count number of lines
def count_lines(text):
  words = get_lines(text)
  return len(words)

# Count total number of diferent words
def count_diferent_words(text):
  words = get_diferent_words(text)
  return len(words)

# Count word distributin - how many times each word appers
def word_distribution(text):
  distribution = {}
  splittext = text.split()
  pont = str.maketrans({key: None for key in string.punctuation})
  for word in splittext:
    if not word.isalpha():
      word = word.translate(pont)
    if word not in distribution:
      distribution[word] = 1
    else:
      distribution[word] = distribution[word] +1
  return distribution

#-- Tagging

# Part of Speech work
def part_of_speech(text):
  words = get_diferent_words(text)
  text = ''
  for word in words:
    text += word + ' '
  tagger = nlpnet.POSTagger('./pos-pt', language='pt')
  return tagger.tag(text)

# Ignore ART PRO-KS PREP+PRO-KS PREP+PROADJ PREP+ADV PREP+PROPSUB PREP+ART PREP
def remove_unwanted_pos(text):
  text = part_of_speech(text)[0]
  unwanted_pos = {"ART", "PRO-KS", "PREP+PRO-KS", "PREP+PROADJ", "PREP+ADV", "PREP+PROPSUB", "PREP+ART", "PREP"}
  for tup in text:
    if tup[1] in unwanted_pos:
      text.remove(tup)
  return text

#--- Do it
text = get_text_from_file()

print(text)
print("\nwords ", count_words(text),": ", get_words(text))
print("\nlines ", count_lines(text),": ", get_lines(text))
print("\ndiferent words ", count_diferent_words(text),": ", get_diferent_words(text))
print("\nword distribution: ",  word_distribution(text))
print("\nTagging: ",  part_of_speech(text)[0])
print("\nRemoved unwanted POS: ", remove_unwanted_pos(text))
